---
output: github_document
---

# Prediction

Outcome to be predicted: $Y_i$

> *example:* a worker's log wage

Characteristics (aka **features**): $X_i=\left(X_{1i},\ldots,X_{pi}\right)'$

> *example:* education, age, state of birth, parents' education, cognitive ability, family background

```{r}
library(tidyverse)
library(fixest)
library(glmnet)
library(patchwork)
library(caTools)
library(glue)

nlsy = read_csv('https://github.com/Mixtape-Sessions/Machine-Learning/blob/main/Labs/data/nlsy97.csv?raw=true')
nlsy = nlsy |>
  drop_na(educ)
```

## Least squares benchmark

```{r}
# polynomial of order 10 for education
for (i in 1:10) {
  var = paste0("educ", i)

  # generate polynomials
  nlsy[[var]] = nlsy$educ^i

  # standardize our X matrix (doesn't matter for OLS, 
  # but will matter for lasso below)
  nlsy[[var]] = nlsy[[var]] |> 
    scale() |> 
    as.numeric()
}

reg = lm(
  lnw_2016 ~ educ1 + educ2 + educ3 + educ4 + educ5 + educ6 + educ7 + 
    educ8 + educ9 + educ10, 
  data = nlsy
)

nlsy$yhat= predict(reg)
```

```{r}
# plot predicted values
summ = nlsy |> 
  group_by(
    educ, educ1, educ2, educ3, educ4, educ5, 
    educ6, educ7, educ8, educ9, educ10
  ) |> 
  summarize(
    mean_y = mean(lnw_2016),
    yhat_reg = mean(yhat)
  ) |> 
  ungroup()

ggplot(summ) + 
  geom_point(
    aes(x = educ, y = mean_y),
    color = "blue", size = 2
  ) + 
  geom_line(
    aes(x = educ, y = yhat_reg), 
    color = "green", size = 1.5
  ) + 
  labs(
    title = "ln Wages by Education in the NLSY",
    x = "Years of Schooling",
    y = "ln Wages"
  ) +
  theme_bw()

```

As we can see, least squares linear regression can approximate any continuous function and can certainly be used for prediction. Include a rich enough set of transformations, and OLS predictions will yield unbiased estimates of the true ideal predictor, the conditional expectation function. But these estimates will be quite noisy. Penalized regression can greatly reduce the variance, at the expense of some bias. But if the bias reduction is great enough, the predictions can have lower MSE. Back to the whiteboard!


## Lasso in action

Welcome back! Let's see lasso in action:

```{r}
X = as.matrix(nlsy[, paste0("educ", 1:10)])
y = nlsy[["lnw_2016"]]

lasso1 = glmnet(x = X, y = y, alpha = 0.001)
lasso2 = glmnet(x = X, y = y, alpha = 0.01)

newX = as.matrix(summ[, paste0("educ", 1:10)])

# Random value of lambda [, 70]
summ$yhat_lasso1 = predict(lasso1, newx = newX)[, 70]
summ$yhat_lasso2 = predict(lasso2, newx = newX)[, 70]
```

Plot results

```{r}

# Coefficient plots ------------------------------------------------------------

n <- names(reg$coefficients)
c <- reg$coefficients
p <- ggplot() +
  geom_col(
    aes(x = n, y = c)
  ) +
  coord_flip() +
  labs(y = "coef.", x = "", title = "OLS") + 
  theme_bw()

n1 <- rownames(coefficients(lasso1))
c1 <- coefficients(lasso1)[,1] |> as.numeric()
p1 <- ggplot() +
  geom_col(
    aes(x = n1, y = c1)
  ) +
  coord_flip() +
  labs(
    y = "coef.", x = "", 
    title = "LASSO with alpha = 0.001"
  ) + 
  theme_bw()

n2 <- rownames(coefficients(lasso2))
c2 <- coefficients(lasso2)[,1] |> as.numeric()
p2 <- ggplot() +
  geom_col(
    aes(x = n2, y = c2)
  ) +
  coord_flip() +
  labs(
    y = "coef.", x = "", 
    title = "LASSO with alpha = 0.01"
  ) + 
  theme_bw()


# Prediction plots -------------------------------------------------------------

f <- ggplot(summ) +
  geom_point(
    aes(x = educ1, y = mean_y),
    color = "black"
  ) +
  geom_point(
    aes(x = educ1, y = yhat_reg),
    color = "red"
  ) +
  geom_line(
    aes(x = educ1, y = yhat_reg),
    color = "red"
  ) +
  labs(
    title = "OLS",
    x = "Years of Schooling",
    y = "ln Wages"
  ) + 
  theme_bw()

# LASSO V1
f1 <- ggplot(summ) +
  geom_point(
    aes(x = educ1, y = mean_y),
    color = "black"
  ) +
  geom_point(
    aes(x = educ1, y = yhat_lasso1),
    color = "red"
  ) +
  geom_line(
    aes(x = educ1, y = yhat_lasso1),
    color = "red"
  ) +
  labs(
    title = "LASSO with alpha = 0.001",
    x = "Years of Schooling",
    y = "ln Wages"
  ) +
  theme_bw()

# LASSO V2
f2 <- ggplot(summ) +
  geom_point(
    aes(x = educ1, y = mean_y),
    color = "black"
  ) +
  geom_point(
    aes(x = educ1, y = yhat_lasso2),
    color = "red"
  ) +
  geom_line(
    aes(x = educ1, y = yhat_lasso2),
    color = "red"
  ) +
  labs(
    title = "LASSO with alpha = 0.01",
    x = "Years of Schooling",
    y = "ln Wages"
  ) +
  theme_bw()

(p + p1 + p2) / (f + f1 + f2)

```

Play around with different values for alpha to see how the fit changes!


### Data-driven tuning parameters: Cross-validation

Quick trip back to the whiteboard!

```{r}
# Automatically does cross-validation in R
cvfit <- cv.glmnet(x = X, y = y, alpha = 1)
plot(cvfit)
cvfit$lambda.min
```

### Lasso-guided variable selection

For illustrative purposes we've been using lasso to determine the functional form for a single underlying regressor: education. But lasso's real power comes in selecting among a large number of regressors.

```{r}
set.seed(123)
split <- sample.split(nlsy$lnw_2016, SplitRatio = 0.8)
training_set <- subset(nlsy, split == TRUE)
test_set <- subset(nlsy, split == FALSE)

y_train <- training_set[["lnw_2016"]]
Y_test <- test_set[["lnw_2016"]]

X_train_scaled <- training_set |> 
  select(-lnw_2016, -exp, -yhat) |>
  scale()
X_test_scaled <- test_set |> 
  select(-lnw_2016, -exp, -yhat) |>
  scale()

# deals with zero-variance cols
X_train_scaled[is.nan(X_train_scaled)] <- 0 
X_test_scaled[is.nan(X_test_scaled)] <- 0 

# TAKE OPTIMAL LAMBDA FROM CVFIT
cvfit <- cv.glmnet(x = X_train_scaled, y = y_train, alpha = 1)
plot(cvfit)

# using minimum lambda
lasso_opt <- glmnet(x = X_train_scaled, y = y_train, alpha = 1, lambda = cvfit$lambda.min) # lambda is penalty
lasso_opt

# y_hat
rsq_train = cor(training_set$lnw_2016, training_set$yhat)^2
rsq_test = cor(test_set$lnw_2016, test_set$yhat)^2
cat(glue("Accuracy on training set: {format(rsq_train, digits = 3)}\n"))
cat(glue("Accuracy on test set: {format(rsq_test, digits = 3)}\n"))

# R^2
cor(training_set$lnw_2016, training_set$yhat)^2
cor(test_set$lnw_2016, test_set$yhat)^2

# coefficients
coefs_lasso = data.frame(
  var = rownames(coef(lasso_opt)), 
  coef_lasso = as.numeric(coef(lasso_opt))
)
subset(coefs_lasso, coef_lasso > 0)
```


To try on your own: load the Oregon HIE data from earlier and try lassoing the OLS regression we did there. What do you notice?


```{r}
# Load Oregon HIE Data

```


## Ridge regression

First, whiteboard. Ridge is another flavor of penalized regression, like lasso. But unlike lasso, ridge penalizes the squares (not the absolute values) of the coefficients. As a result, ridge shrinks coefficients toward zero, but not all the way. Let's give it a try.

```{r}
# TAKE OPTIMAL LAMBDA FROM CVFIT
cvfit <- cv.glmnet(x = X_train_scaled, y = y_train, alpha = 0)

# using minimum lambda
lasso_opt <- glmnet(x = X_train_scaled, y = y_train, alpha = 0, lambda = cvfit$lambda.min) # lambda is penalty
lasso_opt

# y_hat
training_set$yhat <- predict(lasso_opt, newx = X_train_scaled)[, 1]
test_set$yhat <- predict(lasso_opt, newx = X_test_scaled)[, 1]

# R^2
rsq_train = cor(training_set$lnw_2016, training_set$yhat)^2
rsq_test = cor(test_set$lnw_2016, test_set$yhat)^2
cat(glue("Accuracy on training set: {format(rsq_train, digits = 3)}\n"))
cat(glue("Accuracy on test set: {format(rsq_test, digits = 3)}\n"))

# coefficients
coefs_ridge = data.frame(
  var = rownames(coef(lasso_opt)), 
  coef_ridge = as.numeric(coef(lasso_opt))
)
subset(coefs_ridge, coef_ridge > 0)
```

### ...

What do we learn about the relative performance of Lasso and Ridge in this setting? What could be the explanation?

One way to compare Lasso and Ridge, is to visualize their coefficients:

```{r}
coefs = left_join(coefs_lasso, coefs_ridge, by = "var")
coefs$index = 1:nrow(coefs)
coefs = pivot_longer(coefs, 
  cols = c(coef_lasso, coef_ridge),
  names_pattern = "coef_(.*)",
  names_to = "estimator", values_to = "coef"
)

ggplot(subset(coefs, var != "(Intercept)")) + 
  geom_point(
    aes(x = index, y = coef, color = estimator)
  ) + 
  theme_bw()
```

## Elastic Net: best of both worlds?

Elastic net combines lasso and ridge penalization. First, a bit of whiteboard, then let's give it a try.

```{r}
alphas <- c(0.1, 0.5, 0.7, 0.9, 0.95, 0.99)

# Try various values of alpha
for (a in alphas) {
  cvfit <- cv.glmnet(x = X_train_scaled, y = y_train, alpha = a)
  print(a)
  print("best fit...")
  print(cvfit$cvm[cvfit$lambda == cvfit$lambda.min])
  print("--------------")
}
```

```{r}
# TAKE OPTIMAL LAMBDA FROM CVFIT
cvfit <- cv.glmnet(x = X_train_scaled, y = y_train, alpha = 0.9)

# using minimum lambda
lasso_opt <- glmnet(x = X_train_scaled, y = y_train, alpha = 0.9, lambda = cvfit$lambda.min) # lambda is penalty
lasso_opt

# y_hat
training_set$yhat <- predict(lasso_opt, newx = X_train_scaled)[, 1]
test_set$yhat <- predict(lasso_opt, newx = X_test_scaled)[, 1]

# R^2
rsq_train = cor(training_set$lnw_2016, training_set$yhat)^2
rsq_test = cor(test_set$lnw_2016, test_set$yhat)^2
cat(glue("Accuracy on training set: {format(rsq_train, digits = 3)}\n"))
cat(glue("Accuracy on test set: {format(rsq_test, digits = 3)}\n"))

# coefficients
coefs_elastic = data.frame(
  var = rownames(coef(lasso_opt)), 
  coef_elastic = as.numeric(coef(lasso_opt))
)
subset(coefs_elastic, coef_elastic > 0)
```

### ...

Not surprisingly, it doesn't look terribly different from lasso.


## Decision Trees and Random Forests


Let's use random forests to predict wages in the NLSY, just as we did for Lasso, Ridge, and Elastic net. Try it on your own!
```{r}
forest = randomForest(x = X_train_scaled, y = y_train)
plot(forest)

# y_hat
training_set$yhat <- predict(forest, newdata = X_train_scaled)
test_set$yhat <- predict(forest, newdata = X_test_scaled)

# R^2
rsq_train = cor(training_set$lnw_2016, training_set$yhat)^2
rsq_test = cor(test_set$lnw_2016, test_set$yhat)^2
cat(glue("Accuracy on training set: {format(rsq_train, digits = 3)}\n"))
cat(glue("Accuracy on test set: {format(rsq_test, digits = 3)}\n"))

```

How does Random Forest compare with Lasso?


