{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Pr-hNx47pl6"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKjVXkOq5RvM"
      },
      "source": [
        "# Where ML Fits into Causal Inference (review)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-KSl0ixW31A"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mixtape-Sessions/Machine-Learning/blob/main/Labs/python/Causal%20via%20Prediction.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLMqhUEDwRzf"
      },
      "source": [
        "The traditional go-to tool for causal inference is multiple regression:\n",
        "$$\n",
        "Y_i = \\delta D_i + X_i'\\beta+\\varepsilon_i,\n",
        "$$\n",
        "where $D_i$ is the \"treatment\" or causal variable whose effects we are interested in, and $X_i$ is a vector of controls, conditional on which we are willing to assume $D_i$ is as good as randomly assigned.\n",
        "\n",
        "\n",
        "> *example:* Suppose we are interested in the magnitude of racial discrimination in the labor market. One way to conceptualize this is the difference in earnings between two workers who are identical in productivity, but differ in their race, or, the \"effect\" of race. Then $D_i$ would be an indicator for, say, a Black worker. $Y_i$ would be earnings, and $X_i$ would be characteristics that capture determinants of productivity, including educational attainment, cognitive ability, and other background characteristics.\n",
        "\n",
        "Where does machine learning fit into causal inference? It might be tempting to treat\n",
        "this regression as a prediction exercise where we are predicting $Y_{i}$\n",
        "given $D_{i}$ and $X_{i}$. Don't give in to this temptation. We are not\n",
        "after a prediction for $Y_{i}$, we are after a coefficient on $D_{i}$.\n",
        "Modern machine learning algorithms are finely tuned for producing\n",
        "predictions, but along the way they compromise coefficients. So how can we\n",
        "deploy machine learning in the service of estimating the causal coefficient $\\delta $?\n",
        "\n",
        "To see where ML fits in, first remember that an equivalent way to estimate $%\n",
        "\\delta $ is the following three-step procedure:\n",
        "\n",
        "\n",
        "1.   Regress $Y_{i}$ on $X_{i}$ and compute the residuals, $\\tilde{Y}%\n",
        "_{i}=Y_{i}-\\hat{Y}_{i}^{OLS}$, where $\\hat{Y}_{i}^{OLS}=X_{i}^{\\prime\n",
        "}\\left( X^{\\prime }X\\right) ^{-1}X^{\\prime }Y$\n",
        "2.   Regress $D_{i}$ on $X_{i}$ and compute the residuals, $\\tilde{D}%\n",
        "_{i}=D_{i}-\\hat{D}_{i}^{OLS}$, where $\\hat{D}_{i}^{OLS}=X_{i}^{\\prime\n",
        "}\\left( X^{\\prime }X\\right) ^{-1}X^{\\prime }D$\n",
        "\n",
        "3. Regress $\\tilde{Y}_{i}$ on $\\tilde{D}_{i}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGpe2nLBpTjr"
      },
      "source": [
        "Steps 1 and 2 are prediction exercises--ML's wheelhouse. When OLS isn't the right tool for the job, we can replace OLS in those steps with machine learning:\n",
        "\n",
        "1.   Predict $Y_{i}$ based on $X_{i}$ using ML and compute the residuals, $\\tilde{Y}%\n",
        "_{i}=Y_{i}-\\hat{Y}_{i}^{ML}$, where $\\hat{Y}_{i}^{ML}$ is the prediction from an ML algorithm\n",
        "2.   Predict $D_{i}$ based on $X_{i}$ using ML and compute the residuals, $\\tilde{D}%\n",
        "_{i}=D_{i}-\\hat{D}_{i}^{ML}$, where $\\hat{D}_{i}^{ML}$ is the prediction from an ML algorithm\n",
        "\n",
        "3. Regress $\\tilde{Y}_{i}$ on $\\tilde{D}_{i}$.\n",
        "\n",
        "This is the basis for the two major methods we'll look at today: The first is \"Post-Double Selection Lasso\" (Belloni, Chernozhukov, Hansen). The second is \"Double-Debiased Machine Learning\" (Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, Robins)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YNUJiC_pGHK"
      },
      "source": [
        "# Post Double Selection Lasso (PDS Lasso)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrGtZGTDbwUN"
      },
      "source": [
        "## Load useful packages:\n",
        "pandas, numpy, linear_model (from sklearn), and KFold (from sklearn.model_selection)\n",
        "\n",
        "Try it yourself first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSklrnF3swnG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg1D6fc7sprb"
      },
      "source": [
        "### Cheat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK3RIvwKbwUN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import KFold\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1aemlN8bwUO"
      },
      "source": [
        "## Read in data and have a look at it\n",
        "We'll use the NLSY data from yesterday\n",
        "\n",
        "Try it yourself"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX8BER-qtIPh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPXNaDC5tIqx"
      },
      "source": [
        "### Cheat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7M4VPND9bwUO"
      },
      "outputs": [],
      "source": [
        "nlsy=pd.read_csv('https://github.com/Mixtape-Sessions/Machine-Learning/blob/main/Labs/data/nlsy97.csv?raw=true')\n",
        "nlsy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTkTBC_GbwUO"
      },
      "source": [
        "## Define outcome, regressor of interest\n",
        "y = lnw_2016\n",
        "\n",
        "d = black\n",
        "\n",
        "Try it yourself:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEDF-ieJt8r6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GorEJmAdt9y6"
      },
      "source": [
        "### Cheat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9yDNfWbbwUO"
      },
      "outputs": [],
      "source": [
        "y=nlsy['lnw_2016']\n",
        "d=nlsy[['black']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiITTR5XfOPB"
      },
      "source": [
        "## Simple Regression with no Controls\n",
        "Regress y on d and print out coefficient\n",
        "Try it yourself"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LOBDO9nulyf"
      },
      "outputs": [],
      "source": [
        "# instantiate and fit a linear regression object\n",
        "\n",
        "# print out regression coefficient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-flkjNb1umRv"
      },
      "source": [
        "### Cheat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lawwWrR0bwUO"
      },
      "outputs": [],
      "source": [
        "lm=linear_model.LinearRegression().fit(d,y)\n",
        "print(\"Simple regression race gap: {:.3f}\".format(lm.coef_[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10ZQ5f0Q5c-X"
      },
      "source": [
        "### ...\n",
        "Is this the effect we're looking for?\n",
        "\n",
        "Let's try a regression where we control for a few things: education (linearly), experience (linearly), and cognitive ability (afqt, linearly).\n",
        "\n",
        "Try it yourself!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNHm2Mwg52RK"
      },
      "outputs": [],
      "source": [
        "# define X, matrix of the d and the controls we want\n",
        "\n",
        "# run regression\n",
        "\n",
        "# print out coefficient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Jf3kS226D9G"
      },
      "source": [
        "### Cheat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLO-WdCR6FtP"
      },
      "outputs": [],
      "source": [
        "# define RHS, matrix of the d and the controls we want\n",
        "RHS=nlsy[['black','educ','exp','afqt']]\n",
        "# run regression\n",
        "lm.fit(RHS,y)\n",
        "# print out coefficient\n",
        "print(\"Multiple regression-adjusted race gap: {:.3f}\".format(lm.coef_[0]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwwdXTfI6sRS"
      },
      "source": [
        "\n",
        "###...\n",
        "How does it compare to the simple regression?\n",
        "\n",
        "But who is to say the controls we included are sufficient? We have a whole host (hundred!) of other potential controls, not to mention that perhaps the controls we did put in enter linearly. This is a job for ML!\n",
        "\n",
        "To prep, let's define a matrix X with all of our potential controls:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pcdH08j7M-f"
      },
      "outputs": [],
      "source": [
        "X=nlsy.drop(columns=['lnw_2016','black'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyLr9G2BbwUP"
      },
      "source": [
        "## Post Double Selection Lasso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GV680gb4fH9J"
      },
      "source": [
        "### Step 1: Lasso the outcome on X\n",
        "Try it yourself. Don't forget to standardize Xs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vrw7Jp9NvRob"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XBPxADbvSQx"
      },
      "source": [
        "#### Cheat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utBluzYMbwUP"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler().fit(X)\n",
        "X = pd.DataFrame(data=scaler.transform(X),columns = X.columns)\n",
        "\n",
        "lassoy = linear_model.LassoCV(max_iter=1000).fit(X, y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayb8h1TabwUP"
      },
      "source": [
        "### Step 2: Lasso the treatment on X\n",
        "Try it yourself"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1jSm4SivZ9x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y74HbxclvaZM"
      },
      "source": [
        "#### Cheat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8TQIG_MbwUP"
      },
      "outputs": [],
      "source": [
        "lassod = linear_model.LassoCV(max_iter=1000).fit(X, d)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRT_TCtZbwUP"
      },
      "source": [
        "### Step 3: Form the union of controls\n",
        "Try it yourself"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILtNuVVxvpon"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91aP0B-AvqB2"
      },
      "source": [
        "#### Cheat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Soyb1kuFbwUP"
      },
      "outputs": [],
      "source": [
        "Xunion=X.iloc[:,(lassod.coef_!=0) + (lassoy.coef_!=0)]\n",
        "Xunion.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Anw8-QibwUP"
      },
      "source": [
        "### Concatenate treatment with union of controls and regress y on that and print out estimate\n",
        "Try yourself"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nr7yq0ysv83g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm4vZXN-v9Ou"
      },
      "source": [
        "#### Cheat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm3RT7ObbwUP"
      },
      "outputs": [],
      "source": [
        "rhs=pd.concat([d,Xunion],axis=1)\n",
        "fullreg=linear_model.LinearRegression().fit(rhs,y)\n",
        "print(\"PDS regression earnings race gap: {:.3f}\".format(fullreg.coef_[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ub3mDLFbwUP"
      },
      "source": [
        "## Double-Debiased Machine Learning\n",
        "For simplicity, we will first do it without sample splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfsIjhZLjRSc"
      },
      "source": [
        "### Step 1: Ridge outcome on Xs, get residuals\n",
        "Try yourself"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfbKeMvywfZr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLEeasnUwft7"
      },
      "source": [
        "#### Cheat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da9pdvv1leOC"
      },
      "outputs": [],
      "source": [
        "ridgey = linear_model.RidgeCV().fit(X, y)\n",
        "yresid=y-ridgey.predict(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njrC3DP7l83X"
      },
      "source": [
        "### Step 2: Ridge treatment on Xs, get residuals\n",
        "Try yourself"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n19zD9iwmOq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2-anrDxwp9l"
      },
      "source": [
        "#### Cheat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wkcwyi8ImBNa"
      },
      "outputs": [],
      "source": [
        "ridged = linear_model.RidgeCV().fit(X, d)\n",
        "dresid=d-ridged.predict(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "938Uw1dXmKop"
      },
      "source": [
        "### Step 3: Regress y resids on d resids and print out estimate\n",
        "Try yourself"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1g-x_9DwwUl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqbex0fqwxIy"
      },
      "source": [
        "####Cheat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cp1OX-LwmPUL"
      },
      "outputs": [],
      "source": [
        "dmlreg=linear_model.LinearRegression().fit(dresid,yresid)\n",
        "print(\"DML regression earnings race gap: {:.3f}\".format(dmlreg.coef_[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea6hN-cTmY_U"
      },
      "source": [
        "### The real thing: with sample splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUaFlxFQmp1d"
      },
      "outputs": [],
      "source": [
        "# create our sample splitting \"object\"\n",
        "kf = KFold(n_splits=5,shuffle=True,random_state=42)\n",
        "\n",
        "# apply the splits to our Xs\n",
        "kf.get_n_splits(X)\n",
        "\n",
        "# initialize columns for residuals\n",
        "yresid = y*0\n",
        "dresid = d*0\n",
        "\n",
        "# Now loop through each fold\n",
        "for train_index, test_index in kf.split(X):\n",
        "  X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
        "  y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "  d_train, d_test = d.iloc[train_index,:], d.iloc[test_index,:]\n",
        "\n",
        "  # Do DML thing\n",
        "  # Ridge y on training folds:\n",
        "  ridgey.fit(X_train, y_train)\n",
        "\n",
        "  # but get residuals in test set\n",
        "  yresid.iloc[test_index]=y_test-ridgey.predict(X_test)\n",
        "\n",
        "  #Ridge d on training folds\n",
        "  ridged.fit(X_train, d_train)\n",
        "  #but get residuals in test set\n",
        "  dresid.iloc[test_index,:]=d_test-ridged.predict(X_test)\n",
        "\n",
        "\n",
        "# Regress resids\n",
        "dmlreg=linear_model.LinearRegression().fit(dresid,yresid)\n",
        "\n",
        "print(\"DML regression earnings race gap: {:.3f}\".format(dmlreg.coef_[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L91ckWu5CD3L"
      },
      "source": [
        "You want standard errors, do you?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DK0VmmtqBBk7"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "rhs = sm.add_constant(dresid)\n",
        "model = sm.OLS(yresid, rhs)\n",
        "results = model.fit(cov_type='HC3')\n",
        "print(results.summary())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j0INc4lqc2d"
      },
      "source": [
        "## Now do DML using Random Forest!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izctK-2dxxpv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg5Rf4zpM2ax"
      },
      "source": [
        "### Cheat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNBa7c1QM4RQ"
      },
      "outputs": [],
      "source": [
        "# import random forest\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# instantiate random forest objects\n",
        "rfy=RandomForestRegressor(n_estimators=100)\n",
        "rfd = rfy\n",
        "\n",
        "# create our sample splitting \"object\"\n",
        "kf = KFold(n_splits=5,shuffle=True,random_state=42)\n",
        "\n",
        "# apply the splits to our Xs\n",
        "kf.get_n_splits(X)\n",
        "\n",
        "\n",
        "# initialize columns for residuals\n",
        "yresidrf = y*0\n",
        "dresidrf = d*0\n",
        "\n",
        "# Now loop through each fold\n",
        "for train_index, test_index in kf.split(X):\n",
        "  X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
        "  y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "  d_train, d_test = d.iloc[train_index,:], d.iloc[test_index,:]\n",
        "\n",
        "  # Do DML thing\n",
        "  # Ridge y on training folds:\n",
        "  rfy.fit(X_train, y_train)\n",
        "\n",
        "  # but get residuals in test set\n",
        "  yresidrf.iloc[test_index]=y_test-rfy.predict(X_test)\n",
        "\n",
        "  #Ridge d on training folds\n",
        "  rfd.fit(X_train, d_train)\n",
        "  #but get residuals in test set\n",
        "  dresidrf.iloc[test_index,:]=d_test-rfd.predict(X_test).reshape(-1,1)\n",
        "\n",
        "\n",
        "# Regress resids\n",
        "dmlreg=linear_model.LinearRegression().fit(dresidrf,yresidrf)\n",
        "\n",
        "print(\"DML (Random forest) regression earnings race gap: {:.3f}\".format(dmlreg.coef_[0]))\n",
        "rhs = sm.add_constant(dresidrf)\n",
        "model = sm.OLS(yresidrf, rhs)\n",
        "results = model.fit(cov_type='HC3')\n",
        "print(results.summary())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Wg1D6fc7sprb",
        "yPXNaDC5tIqx",
        "GorEJmAdt9y6",
        "-flkjNb1umRv",
        "4Jf3kS226D9G",
        "7XBPxADbvSQx",
        "y74HbxclvaZM",
        "91aP0B-AvqB2",
        "Tm4vZXN-v9Ou",
        "iLEeasnUwft7",
        "P2-anrDxwp9l",
        "aqbex0fqwxIy",
        "cg5Rf4zpM2ax"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
